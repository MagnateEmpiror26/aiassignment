{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgs4FzhSywMY",
    "outputId": "28a5dcaf-48c2-478e-c531-349ea0a23f2d"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew66EgUW1D5h"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r')\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSLL3EZNP7zS"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "  # split into tokens by white space\n",
    "  tokens = doc.split()\n",
    "  # prepare regex for char filtering\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "  # remove punctuation from each word\n",
    "  tokens = [re_punc.sub('', w) for w in tokens]\n",
    "  # remove remaining tokens that are not alphabetic\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  # filter out stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [w for w in tokens if not w in stop_words]\n",
    "  # filter out short tokens\n",
    "  tokens = [word for word in tokens if len(word) > 1]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpcEkAKATxag",
    "outputId": "dae1848a-50cb-4c6a-ba08-f50f6c594cbd"
   },
   "outputs": [],
   "source": [
    "filename = '../input/movie-review-dataset/txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCAGvOMWUs-3"
   },
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "  # load doc\n",
    "  doc = load_doc(filename)\n",
    "  # clean doc\n",
    "  tokens = clean_doc(doc)\n",
    "  # update counts\n",
    "  vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kt1XaUciUxHV"
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "  # walk through all files in the folder\n",
    "  for filename in listdir(directory):\n",
    "  # skip any reviews in the test set\n",
    "    if filename.startswith('cv9'):\n",
    "      continue\n",
    "    # create the full path of the file to open\n",
    "    path = directory + '/' + filename\n",
    "    # add doc to vocab\n",
    "    add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnRper5CVDc3",
    "outputId": "894fcf78-9cea-4fb4-ea50-614a6e5b92c1"
   },
   "outputs": [],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('../input/movie-review-dataset/txt_sentoken/pos', vocab)\n",
    "process_docs('../input/movie-review-dataset/txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsL7ppzuVS99",
    "outputId": "09a7a52c-1c7f-4dd8-8052-427548dbabf4"
   },
   "outputs": [],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurrence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goSNbMjOWF06"
   },
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "  # convert lines to a single blob of text\n",
    "  data = '\\n'.join(lines)\n",
    "  # open file\n",
    "  file = open(filename, 'w')\n",
    "  # write text\n",
    "  file.write(data)\n",
    "  # close file\n",
    "  file.close()\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLRJzeTzZEfD"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "  # split into tokens by white space\n",
    "  tokens = doc.split()\n",
    "  # prepare regex for char filtering\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "  # remove punctuation from each word\n",
    "  tokens = [re_punc.sub('', w) for w in tokens]\n",
    "  # filter out tokens not in vocab\n",
    "  tokens = [w for w in tokens if w in vocab]\n",
    "  tokens = ' '.join(tokens)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mH_Ow_M2ZwWb"
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "  documents = list()\n",
    "  # walk through all files in the folder\n",
    "  for filename in listdir(directory):\n",
    "    # skip any reviews in the test set\n",
    "    if is_train and filename.startswith('cv9'):\n",
    "      continue\n",
    "    if not is_train and not filename.startswith('cv9'):\n",
    "      continue\n",
    "    # create the full path of the file to open\n",
    "    path = directory + '/' + filename\n",
    "    # load the doc\n",
    "    doc = load_doc(path)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc, vocab)\n",
    "    # add to list\n",
    "    documents.append(tokens)\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_ed-RBRaBa3"
   },
   "outputs": [],
   "source": [
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "  # load documents\n",
    "  neg = process_docs('../input/movie-review-dataset/txt_sentoken/neg', vocab, is_train)\n",
    "  pos = process_docs('../input/movie-review-dataset/txt_sentoken/pos', vocab, is_train)\n",
    "  docs = neg + pos\n",
    "  # prepare labels\n",
    "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "  return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ846ND7aQ4d"
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lago6VY2bfYk"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6_r1SfkaiBO"
   },
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "  # integer encode\n",
    "  encoded = tokenizer.texts_to_sequences(docs)\n",
    "  # pad sequences\n",
    "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "  return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcB6NNdvbv5z",
    "outputId": "100e2c4c-fb86-44b1-c250-6623a2c56c48"
   },
   "outputs": [],
   "source": [
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piiVY9z2asWW"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "  model.add(Conv1D(32, 8, activation='relu'))\n",
    "  model.add(MaxPooling1D(2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  # compile network\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  # summarize defined model\n",
    "  model.summary()\n",
    "  plot_model(model, to_file='model.png', show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG5xf86vOfsR"
   },
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsz6Fgeda01R"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbn6trNLbQAq"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjEAQukyQGn6"
   },
   "outputs": [],
   "source": [
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "  # clean review\n",
    "  line = clean_doc(review, vocab)\n",
    "  # encode and pad review\n",
    "  padded = encode_docs(tokenizer, max_length, [line])\n",
    "  # predict sentiment\n",
    "  yhat = model.predict(padded, verbose=0)\n",
    "  # retrieve predicted percentage and label\n",
    "  percent_pos = yhat[0,0]\n",
    "  if round(percent_pos) == 0:\n",
    "    return (1-percent_pos), 'NEGATIVE'\n",
    "  return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test positive text\n",
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
